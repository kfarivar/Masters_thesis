@misc{underspecification,
	doi = {10.48550/ARXIV.2011.03395},
	
	url = {https://arxiv.org/abs/2011.03395},
	
	author = {D'Amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew D. and Hormozdiari, Farhad and Houlsby, Neil and Hou, Shaobo and Jerfel, Ghassen and Karthikesalingam, Alan and Lucic, Mario and Ma, Yian and McLean, Cory and Mincu, Diana and Mitani, Akinori and Montanari, Andrea and Nado, Zachary and Natarajan, Vivek and Nielson, Christopher and Osborne, Thomas F. and Raman, Rajiv and Ramasamy, Kim and Sayres, Rory and Schrouff, Jessica and Seneviratne, Martin and Sequeira, Shannon and Suresh, Harini and Veitch, Victor and Vladymyrov, Max and Wang, Xuezhi and Webster, Kellie and Yadlowsky, Steve and Yun, Taedong and Zhai, Xiaohua and Sculley, D.},
	
	keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {Underspecification Presents Challenges for Credibility in Modern Machine Learning},
	
	publisher = {arXiv},
	
	year = {2020},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{adv_exp_original_paper,
  doi = {10.48550/ARXIV.1312.6199},
  
  url = {https://arxiv.org/abs/1312.6199},
  
  author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Intriguing properties of neural networks},
  
  publisher = {arXiv},
  
  year = {2013},
  
  copyright = {Creative Commons Attribution 3.0 Unported}
}


@misc{supervised_texture_bias,
	doi = {10.48550/ARXIV.1811.12231},
	
	url = {https://arxiv.org/abs/1811.12231},
	
	author = {Geirhos, Robert and Rubisch, Patricia and Michaelis, Claudio and Bethge, Matthias and Wichmann, Felix A. and Brendel, Wieland},
	
	keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Neurons and Cognition (q-bio.NC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Biological sciences, FOS: Biological sciences},
	
	title = {ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness},
	
	publisher = {arXiv},
	
	year = {2018},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{contrastive_loss_shortcut_solutions,
	doi = {10.48550/ARXIV.2011.02803},
	
	url = {https://arxiv.org/abs/2011.02803},
	
	author = {Chen, Ting and Luo, Calvin and Li, Lala},
	
	keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {Intriguing Properties of Contrastive Losses},
	
	publisher = {arXiv},
	
	year = {2020},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{contrastive_learning_spectral_proof,
	doi = {10.48550/ARXIV.2106.04156},
	
	url = {https://arxiv.org/abs/2106.04156},
	
	author = {HaoChen, Jeff Z. and Wei, Colin and Gaidon, Adrien and Ma, Tengyu},
	
	keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss},
	
	publisher = {arXiv},
	
	year = {2021},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{lars,
	doi = {10.48550/ARXIV.1708.03888},
	
	url = {https://arxiv.org/abs/1708.03888},
	
	author = {You, Yang and Gitman, Igor and Ginsburg, Boris},
	
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {Large Batch Training of Convolutional Networks},
	
	publisher = {arXiv},
	
	year = {2017},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{simsiam,
	doi = {10.48550/ARXIV.2011.10566},
	
	url = {https://arxiv.org/abs/2011.10566},
	
	author = {Chen, Xinlei and He, Kaiming},
	
	keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {Exploring Simple Siamese Representation Learning},
	
	publisher = {arXiv},
	
	year = {2020},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{ssl_loss_shortcuts,
	author = {Chen, Ting and Luo, Calvin and Li, Lala},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
	pages = {11834--11845},
	publisher = {Curran Associates, Inc.},
	title = {Intriguing Properties of Contrastive Losses},
	url = {https://proceedings.neurips.cc/paper/2021/file/628f16b29939d1b060af49f66ae0f7f8-Paper.pdf},
	volume = {34},
	year = {2021}
}


@article{excessive_invariance,
	doi = {10.48550/ARXIV.1811.00401},
	
	url = {https://arxiv.org/abs/1811.00401},
	
	author = {Jacobsen, Jörn-Henrik and Behrmann, Jens and Zemel, Richard and Bethge, Matthias},
	
	keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {Excessive Invariance Causes Adversarial Vulnerability},
	
	publisher = {arXiv},
	
	year = {2018},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{cure,
	doi = {10.48550/ARXIV.1811.09716},
	
	url = {https://arxiv.org/abs/1811.09716},
	
	author = {Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Uesato, Jonathan and Frossard, Pascal},
	
	keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {Robustness via curvature regularization, and vice versa},
	
	publisher = {arXiv},
	
	year = {2018},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{grad_align,
	doi = {10.48550/ARXIV.2007.02617},
	
	url = {https://arxiv.org/abs/2007.02617},
	
	author = {Andriushchenko, Maksym and Flammarion, Nicolas},
	
	keywords = {Machine Learning (cs.LG), Cryptography and Security (cs.CR), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {Understanding and Improving Fast Adversarial Training},
	
	publisher = {arXiv},
	
	year = {2020},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{goodfellow_fgsm,
	doi = {10.48550/ARXIV.1412.6572},
	
	url = {https://arxiv.org/abs/1412.6572},
	
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	
	keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {Explaining and Harnessing Adversarial Examples},
	
	publisher = {arXiv},
	
	year = {2014},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{contrast_to_divide,
	doi = {10.1109/wacv51458.2022.00046},
	
	url = {https://doi.org/10.1109%2Fwacv51458.2022.00046},
	
	year = 2022,
	month = {jan},
	
	publisher = {{IEEE}
	},
	
	author = {Evgenii Zheltonozhskii and Chaim Baskin and Avi Mendelson and Alex M. Bronstein and Or Litany},
	
	title = {Contrast to Divide: Self-Supervised Pre-Training for Learning with Noisy Labels},
	
	booktitle = {2022 {IEEE}/{CVF} Winter Conference on Applications of Computer Vision ({WACV})}
}


@misc{ELR,
	doi = {10.48550/ARXIV.2007.00151},
	
	url = {https://arxiv.org/abs/2007.00151},
	
	author = {Liu, Sheng and Niles-Weed, Jonathan and Razavian, Narges and Fernandez-Granda, Carlos},
	
	keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {Early-Learning Regularization Prevents Memorization of Noisy Labels},
	
	publisher = {arXiv},
	
	year = {2020},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{adv_training_plus_ssl_training,
	author    = {Dan Hendrycks and
	Mantas Mazeika and
	Saurav Kadavath and
	Dawn Song},
	title     = {Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty},
	journal   = {CoRR},
	volume    = {abs/1906.12340},
	year      = {2019},
	url       = {http://arxiv.org/abs/1906.12340},
	eprinttype = {arXiv},
	eprint    = {1906.12340},
	timestamp = {Mon, 01 Jul 2019 13:00:07 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1906-12340.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{label_noise_contrastive_learning,
	author    = {Yihao Xue and
	Kyle Whitecross and
	Baharan Mirzasoleiman},
	title     = {Investigating Why Contrastive Learning Benefits Robustness Against
	Label Noise},
	journal   = {CoRR},
	volume    = {abs/2201.12498},
	year      = {2022},
	url       = {https://arxiv.org/abs/2201.12498},
	eprinttype = {arXiv},
	eprint    = {2201.12498},
	timestamp = {Wed, 02 Feb 2022 15:00:01 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/abs-2201-12498.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{contrastive_inverts_data_gen,
	doi = {10.48550/ARXIV.2102.08850},
	
	url = {https://arxiv.org/abs/2102.08850},
	
	author = {Zimmermann, Roland S. and Sharma, Yash and Schneider, Steffen and Bethge, Matthias and Brendel, Wieland},
	
	keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {Contrastive Learning Inverts the Data Generating Process},
	
	publisher = {arXiv},
	
	year = {2021},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{simsiam,
	doi = {10.48550/ARXIV.2011.10566},
	
	url = {https://arxiv.org/abs/2011.10566},
	
	author = {Chen, Xinlei and He, Kaiming},
	
	keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {Exploring Simple Siamese Representation Learning},
	
	publisher = {arXiv},
	
	year = {2020},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{bolts,
	title={A Framework For Contrastive Self-Supervised Learning And Designing A New Approach},
	author={Falcon, William and Cho, Kyunghyun},
	journal={arXiv preprint arXiv:2009.00104},
	year={2020}
}


@inproceedings{concentration_measure,
	author = {Mahloujifar, Saeed and Zhang, Xiao and Mahmoody, Mohammad and Evans, David},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Empirically Measuring Concentration: Fundamental Limits on Intrinsic Robustness},
	url = {https://proceedings.neurips.cc/paper/2019/file/46f76a4bda9a9579eab38a8f6eabcda1-Paper.pdf},
	volume = {32},
	year = {2019}
}


@misc{autoattack,
	doi = {10.48550/ARXIV.2003.01690},
	
	url = {https://arxiv.org/abs/2003.01690},
	
	author = {Croce, Francesco and Hein, Matthias},
	
	keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks},
	
	publisher = {arXiv},
	
	year = {2020},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}



@software{huy_phan_2021_4431043,
	author       = {Huy Phan},
	title        = {huyvnphan/PyTorch\_CIFAR10},
	month        = jan,
	year         = 2021,
	publisher    = {Zenodo},
	version      = {v3.0.1},
	doi          = {10.5281/zenodo.4431043},
	url          = {https://doi.org/10.5281/zenodo.4431043}
}


@misc{barlow_twins,
	url = {https://doi.org/10.48550/arxiv.2103.03230},
	doi = {10.48550/ARXIV.2103.03230},
	
	url = {https://arxiv.org/abs/2103.03230},
	
	author = {Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, Stéphane},
	
	keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Neurons and Cognition (q-bio.NC), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Biological sciences, FOS: Biological sciences},
	
	title = {Barlow Twins: Self-Supervised Learning via Redundancy Reduction},
	
	publisher = {arXiv},
	
	year = {2021},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}



@article{ACL_Adversarial_Contrastive_Learning_2020,
	title = {Robust {Pre}-{Training} by {Adversarial} {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2010.13337},
	abstract = {Recent work has shown that, when integrated with adversarial training, self-supervised pre-training can lead to state-of-the-art robustness In this work, we improve robustness-aware self-supervised pre-training by learning representations that are consistent under both data augmentations and adversarial perturbations. Our approach leverages a recent contrastive learning framework, which learns representations by maximizing feature consistency under differently augmented views. This fits particularly well with the goal of adversarial robustness, as one cause of adversarial fragility is the lack of feature invariance, i.e., small input perturbations can result in undesirable large changes in features or even predicted labels. We explore various options to formulate the contrastive task, and demonstrate that by injecting adversarial perturbations, contrastive pre-training can lead to models that are both label-efficient and robust. We empirically evaluate the proposed Adversarial Contrastive Learning (ACL) and show it can consistently outperform existing methods. For example on the CIFAR-10 dataset, ACL outperforms the previous state-of-the-art unsupervised robust pre-training approach by 2.99\% on robust accuracy and 2.14\% on standard accuracy. We further demonstrate that ACL pre-training can improve semi-supervised adversarial training, even when only a few labeled examples are available. Our codes and pre-trained models have been released at: https://github.com/VITA-Group/Adversarial-Contrastive-Learning.},
	urldate = {2021-11-04},
	journal = {arXiv:2010.13337 [cs]},
	author = {Jiang, Ziyu and Chen, Tianlong and Chen, Ting and Wang, Zhangyang},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.13337},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/kiyarash/Zotero/storage/MPT5FWPN/Jiang et al. - 2020 - Robust Pre-Training by Adversarial Contrastive Lea.pdf:application/pdf;arXiv.org Snapshot:/home/kiyarash/Zotero/storage/GLYRKS2F/2010.html:text/html},
}

@article{ifm_ssl_avoid_shortcuts,
	title = {Can contrastive learning avoid shortcut solutions?},
	url = {http://arxiv.org/abs/2106.11230},
	abstract = {The generalization of representations learned via contrastive learning depends crucially on what features of the data are extracted. However, we observe that the contrastive loss does not always sufficiently guide which features are extracted, a behavior that can negatively impact the performance on downstream tasks via "shortcuts", i.e., by inadvertently suppressing important predictive features. We find that feature extraction is influenced by the difficulty of the so-called instance discrimination task (i.e., the task of discriminating pairs of similar points from pairs of dissimilar ones). Although harder pairs improve the representation of some features, the improvement comes at the cost of suppressing previously well represented features. In response, we propose implicit feature modification (IFM), a method for altering positive and negative samples in order to guide contrastive models towards capturing a wider variety of predictive features. Empirically, we observe that IFM reduces feature suppression, and as a result improves performance on vision and medical imaging tasks. The code is available at: {\textbackslash}url\{https://github.com/joshr17/IFM\}.},
	urldate = {2021-10-12},
	journal = {arXiv:2106.11230 [cs]},
	author = {Robinson, Joshua and Sun, Li and Yu, Ke and Batmanghelich, Kayhan and Jegelka, Stefanie and Sra, Suvrit},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.11230},
	keywords = {Computer Science - Machine Learning},
	file = {Robinson et al. - 2021 - Can contrastive learning avoid shortcut solutions.pdf:/home/kiyarash/Zotero/storage/FY3B3ABH/Robinson et al. - 2021 - Can contrastive learning avoid shortcut solutions.pdf:application/pdf;adv_input_vs_IFM.jpg:/home/kiyarash/Zotero/storage/D4CFYYKH/adv_input_vs_IFM.jpg:image/jpeg},
}




@article{arora_theoretical_2019,
	title = {A {Theoretical} {Analysis} of {Contrastive} {Unsupervised} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1902.09229},
	abstract = {Recent empirical works have successfully used unlabeled data to learn feature representations that are broadly useful in downstream classification tasks. Several of these methods are reminiscent of the well-known word2vec embedding algorithm: leveraging availability of pairs of semantically "similar" data points and "negative samples," the learner forces the inner product of representations of similar pairs with each other to be higher on average than with negative samples. The current paper uses the term contrastive learning for such algorithms and presents a theoretical framework for analyzing them by introducing latent classes and hypothesizing that semantically similar points are sampled from the same latent class. This framework allows us to show provable guarantees on the performance of the learned representations on the average classification task that is comprised of a subset of the same set of latent classes. Our generalization bound also shows that learned representations can reduce (labeled) sample complexity on downstream tasks. We conduct controlled experiments in both the text and image domains to support the theory.},
	urldate = {2021-08-01},
	journal = {arXiv:1902.09229 [cs, stat]},
	author = {Arora, Sanjeev and Khandeparkar, Hrishikesh and Khodak, Mikhail and Plevrakis, Orestis and Saunshi, Nikunj},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.09229},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/kiyarash/Zotero/storage/8TB6IGJV/Arora et al. - 2019 - A Theoretical Analysis of Contrastive Unsupervised.pdf:application/pdf},
}

@article{rotation_gidaris_unsupervised_2018,
	title = {Unsupervised {Representation} {Learning} by {Predicting} {Image} {Rotations}},
	url = {http://arxiv.org/abs/1803.07728},
	abstract = {Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training ConvNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them state-of-the-art performance. Specifically, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art approaches in unsupervised representation learning and thus significantly close the gap with supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-of-the-art (among unsupervised methods) mAP of 54.4\% that is only 2.4 points lower from the supervised case. We get similarly striking results when we transfer our unsupervised learned features on various other tasks, such as ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification. The code and models of our paper will be published on: https://github.com/gidariss/FeatureLearningRotNet .},
	urldate = {2021-08-02},
	journal = {arXiv:1803.07728 [cs]},
	author = {Gidaris, Spyros and Singh, Praveer and Komodakis, Nikos},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.07728},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/kiyarash/Zotero/storage/7WUZUYAQ/Gidaris et al. - 2018 - Unsupervised Representation Learning by Predicting.pdf:application/pdf;arXiv.org Snapshot:/home/kiyarash/Zotero/storage/KNPAFQH6/1803.html:text/html},
}

@misc{weng_2019, title={Self-Supervised representation learning}, url={https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html}, journal={Lil'Log}, author={Weng, Lilian}, year={2019}, month={Nov}} 


@article{athalye_obfuscated_2018,
	title = {Obfuscated {Gradients} {Give} a {False} {Sense} of {Security}: {Circumventing} {Defenses} to {Adversarial} {Examples}},
	shorttitle = {Obfuscated {Gradients} {Give} a {False} {Sense} of {Security}},
	url = {http://arxiv.org/abs/1802.00420},
	abstract = {We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimization-based attacks, we find defenses relying on this effect can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three types of obfuscated gradients we discover, we develop attack techniques to overcome it. In a case study, examining non-certified white-box-secure defenses at ICLR 2018, we find obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 completely, and 1 partially, in the original threat model each paper considers.},
	urldate = {2021-10-01},
	journal = {arXiv:1802.00420 [cs]},
	author = {Athalye, Anish and Carlini, Nicholas and Wagner, David},
	month = jul,
	year = {2018},
	note = {arXiv: 1802.00420},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/kiyarash/Zotero/storage/X6PT3PWZ/Athalye et al. - 2018 - Obfuscated Gradients Give a False Sense of Securit.pdf:application/pdf;arXiv.org Snapshot:/home/kiyarash/Zotero/storage/792WTIX4/1802.html:text/html},
}


@article{robustness_at_odds_madry,
	title = {Robustness {May} {Be} at {Odds} with {Accuracy}},
	url = {http://arxiv.org/abs/1805.12152},
	abstract = {We show that there may exist an inherent tension between the goal of adversarial robustness and that of standard generalization. Specifically, training robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy. We demonstrate that this trade-off between the standard accuracy of a model and its robustness to adversarial perturbations provably exists in a fairly simple and natural setting. These findings also corroborate a similar phenomenon observed empirically in more complex settings. Further, we argue that this phenomenon is a consequence of robust classifiers learning fundamentally different feature representations than standard classifiers. These differences, in particular, seem to result in unexpected benefits: the representations learned by robust models tend to align better with salient data characteristics and human perception.},
	urldate = {2021-10-01},
	journal = {arXiv:1805.12152 [cs, stat]},
	author = {Tsipras, Dimitris and Santurkar, Shibani and Engstrom, Logan and Turner, Alexander and Madry, Aleksander},
	month = sep,
	year = {2019},
	note = {arXiv: 1805.12152},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/kiyarash/Zotero/storage/LPJV4SB2/Tsipras et al. - 2019 - Robustness May Be at Odds with Accuracy.pdf:application/pdf;arXiv.org Snapshot:/home/kiyarash/Zotero/storage/TIZ6PBTT/1805.html:text/html},
}

@article{laidlaw_perceptual_2020,
	title = {Perceptual {Adversarial} {Robustness}: {Defense} {Against} {Unseen} {Threat} {Models}},
	shorttitle = {Perceptual {Adversarial} {Robustness}},
	url = {http://arxiv.org/abs/2006.12655},
	abstract = {A key challenge in adversarial robustness is the lack of a precise mathematical characterization of human perception, used in the deﬁnition of adversarial attacks that are imperceptible to human eyes. Most current attacks and defenses try to avoid this issue by considering restrictive adversarial threat models such as those bounded by L2 or L∞ distances, spatial perturbations, etc. However, models that are robust against any of these restrictive threat models are still fragile against other threat models, i.e. they have poor generalization to unforeseen attacks. Moreover, even if a model is robust against the union of several restrictive threat models, it is still susceptible to other imperceptible adversarial examples that are not contained in any of the constituent threat models. To resolve these issues, we propose adversarial training against the set of all imperceptible adversarial examples. Since this set is intractable to compute without a human in the loop, we approximate it using deep neural networks. We call this threat model the neural perceptual threat model (NPTM); it includes adversarial examples with a bounded neural perceptual distance (a neural network-based approximation of the true perceptual distance) to natural images. Through an extensive perceptual study, we show that the neural perceptual distance correlates well with human judgements of perceptibility of adversarial examples, validating our threat model. Under the NPTM, we develop novel perceptual adversarial attacks and defenses. Because the NPTM is very broad, we ﬁnd that Perceptual Adversarial Training (PAT) against a perceptual attack gives robustness against many other types of adversarial attacks. We test PAT on CIFAR-10 and ImageNet-100 against ﬁve diverse adversarial attacks: L2, L∞, spatial, recoloring, and JPEG. We ﬁnd that PAT achieves state-of-the-art robustness against the union of these ﬁve attacks—more than doubling the accuracy over the next best model—without training against any of them. That is, PAT generalizes well to unforeseen perturbation types. This is vital in sensitive applications where a particular threat model cannot be assumed, and to the best of our knowledge, PAT is the ﬁrst adversarial defense with this property.},
	language = {en},
	urldate = {2021-01-14},
	journal = {arXiv:2006.12655 [cs, stat]},
	author = {Laidlaw, Cassidy and Singla, Sahil and Feizi, Soheil},
	month = oct,
	year = {2020},
	note = {arXiv: 2006.12655},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {new_version:/home/kiyarash/Zotero/storage/ECD7VTFJ/2006.12655.pdf:application/pdf},
}


@article{robinson_can_2021,
	title = {Can contrastive learning avoid shortcut solutions?},
	url = {http://arxiv.org/abs/2106.11230},
	abstract = {The generalization of representations learned via contrastive learning depends crucially on what features of the data are extracted. However, we observe that the contrastive loss does not always sufficiently guide which features are extracted, a behavior that can negatively impact the performance on downstream tasks via "shortcuts", i.e., by inadvertently suppressing important predictive features. We find that feature extraction is influenced by the difficulty of the so-called instance discrimination task (i.e., the task of discriminating pairs of similar points from pairs of dissimilar ones). Although harder pairs improve the representation of some features, the improvement comes at the cost of suppressing previously well represented features. In response, we propose implicit feature modification (IFM), a method for altering positive and negative samples in order to guide contrastive models towards capturing a wider variety of predictive features. Empirically, we observe that IFM reduces feature suppression, and as a result improves performance on vision and medical imaging tasks. The code is available at: {\textbackslash}url\{https://github.com/joshr17/IFM\}.},
	urldate = {2021-10-12},
	journal = {arXiv:2106.11230 [cs]},
	author = {Robinson, Joshua and Sun, Li and Yu, Ke and Batmanghelich, Kayhan and Jegelka, Stefanie and Sra, Suvrit},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.11230},
	keywords = {Computer Science - Machine Learning},
	file = {Robinson et al. - 2021 - Can contrastive learning avoid shortcut solutions.pdf:/home/kiyarash/Zotero/storage/FY3B3ABH/Robinson et al. - 2021 - Can contrastive learning avoid shortcut solutions.pdf:application/pdf;adv_input_vs_IFM.jpg:/home/kiyarash/Zotero/storage/D4CFYYKH/adv_input_vs_IFM.jpg:image/jpeg},
}

@article{tishby_deep_2015,
	title = {Deep {Learning} and the {Information} {Bottleneck} {Principle}},
	url = {http://arxiv.org/abs/1503.02406},
	abstract = {Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the information bottleneck (IB) principle. We first show that any DNN can be quantified by the mutual information between the layers and the input and output variables. Using this representation we can calculate the optimal information theoretic limits of the DNN and obtain finite sample generalization bounds. The advantage of getting closer to the theoretical limit is quantifiable both by the generalization bound and by the network's simplicity. We argue that both the optimal architecture, number of layers and features/connections at each layer, are related to the bifurcation points of the information bottleneck tradeoff, namely, relevant compression of the input layer with respect to the output layer. The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve. We believe that this new insight can lead to new optimality bounds and deep learning algorithms.},
	urldate = {2021-10-20},
	journal = {arXiv:1503.02406 [cs]},
	author = {Tishby, Naftali and Zaslavsky, Noga},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.02406},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/kiyarash/Zotero/storage/KF6ZMKD6/Tishby and Zaslavsky - 2015 - Deep Learning and the Information Bottleneck Princ.pdf:application/pdf;arXiv.org Snapshot:/home/kiyarash/Zotero/storage/ZH9HUXVU/1503.html:text/html},
}

@article{kilbertus_generalization_2018,
	title = {Generalization in anti-causal learning},
	url = {http://arxiv.org/abs/1812.00524},
	abstract = {The ability to learn and act in novel situations is still a prerogative of animate intelligence, as current machine learning methods mostly fail when moving beyond the standard i.i.d. setting. What is the reason for this discrepancy? Most machine learning tasks are anti-causal, i.e., we infer causes (labels) from effects (observations). Typically, in supervised learning we build systems that try to directly invert causal mechanisms. Instead, in this paper we argue that strong generalization capabilities crucially hinge on searching and validating meaningful hypotheses, requiring access to a causal model. In such a framework, we want to find a cause that leads to the observed effect. Anti-causal models are used to drive this search, but a causal model is required for validation. We investigate the fundamental differences between causal and anti-causal tasks, discuss implications for topics ranging from adversarial attacks to disentangling factors of variation, and provide extensive evidence from the literature to substantiate our view. We advocate for incorporating causal models in supervised learning to shift the paradigm from inference only, to search and validation.},
	urldate = {2021-10-21},
	journal = {arXiv:1812.00524 [cs, stat]},
	author = {Kilbertus, Niki and Parascandolo, Giambattista and Schölkopf, Bernhard},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.00524},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/kiyarash/Zotero/storage/UK2NXTRN/Kilbertus et al. - 2018 - Generalization in anti-causal learning.pdf:application/pdf;arXiv.org Snapshot:/home/kiyarash/Zotero/storage/TS4UG5AL/1812.html:text/html},
}

@article{shortcut_in_contrastive_losses,
	title = {Intriguing {Properties} of {Contrastive} {Losses}},
	url = {http://arxiv.org/abs/2011.02803},
	abstract = {We study three intriguing properties of contrastive learning. First, we generalize the standard contrastive loss to a broader family of losses, and we find that various instantiations of the generalized loss perform similarly under the presence of a multi-layer non-linear projection head. Second, we study if instance-based contrastive learning (with a global image representation) can learn well on images with multiple objects present. We find that meaningful hierarchical local features can be learned despite the fact that these objectives operate on global instance-level features. Finally, we study the phenomenon of feature suppression among competing features shared across augmented views, such as "color distribution" vs "object class". We construct datasets with explicit and controllable competing features, and show that, for contrastive learning, a few bits of easy-to-learn shared features can suppress, and even fully prevent, the learning of other sets of competing features. In scenarios where there are multiple objects in an image, the dominant object would suppress the learning of smaller objects. Existing contrastive learning methods critically rely on data augmentation to favor certain sets of features over others, and could suffer from learning saturation for scenarios where existing augmentations cannot fully address the feature suppression. This poses open challenges to existing contrastive learning techniques.},
	urldate = {2021-11-04},
	journal = {arXiv:2011.02803 [cs, stat]},
	author = {Chen, Ting and Luo, Calvin and Li, Lala},
	month = oct,
	year = {2021},
	note = {arXiv: 2011.02803},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/kiyarash/Zotero/storage/IL2CAREF/Chen et al. - 2021 - Intriguing Properties of Contrastive Losses.pdf:application/pdf;arXiv.org Snapshot:/home/kiyarash/Zotero/storage/DFVZV3EQ/2011.html:text/html},
}


@article{3dident,
	title = {Self-{Supervised} {Learning} with {Data} {Augmentations} {Provably} {Isolates} {Content} from {Style}},
	url = {http://arxiv.org/abs/2106.04619},
	abstract = {Self-supervised representation learning has shown remarkable success in a number of domains. A common practice is to perform data augmentation via hand-crafted transformations intended to leave the semantics of the data invariant. We seek to understand the empirical success of this approach from a theoretical perspective. We formulate the augmentation process as a latent variable model by postulating a partition of the latent representation into a content component, which is assumed invariant to augmentation, and a style component, which is allowed to change. Unlike prior work on disentanglement and independent component analysis, we allow for both nontrivial statistical and causal dependencies in the latent space. We study the identifiability of the latent representation based on pairs of views of the observations and prove sufficient conditions that allow us to identify the invariant content partition up to an invertible mapping in both generative and discriminative settings. We find numerical simulations with dependent latent variables are consistent with our theory. Lastly, we introduce Causal3DIdent, a dataset of high-dimensional, visually complex images with rich causal dependencies, which we use to study the effect of data augmentations performed in practice.},
	urldate = {2021-12-17},
	journal = {arXiv:2106.04619 [cs, stat]},
	author = {von Kügelgen, Julius and Sharma, Yash and Gresele, Luigi and Brendel, Wieland and Schölkopf, Bernhard and Besserve, Michel and Locatello, Francesco},
	month = oct,
	year = {2021},
	note = {arXiv: 2106.04619},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/kiyarash/Zotero/storage/5JHUQPK6/von Kügelgen et al. - 2021 - Self-Supervised Learning with Data Augmentations P.pdf:application/pdf;arXiv.org Snapshot:/home/kiyarash/Zotero/storage/6UT9L4WL/2106.html:text/html},
}

@misc{adv_training_madry,
	title={Towards Deep Learning Models Resistant to Adversarial Attacks}, 
	author={Aleksander Madry and Aleksandar Makelov and Ludwig Schmidt and Dimitris Tsipras and Adrian Vladu},
	year={2019},
	eprint={1706.06083},
	archivePrefix={arXiv},
	primaryClass={stat.ML}
}

@misc{features_not_bugs_madry,
	
	doi = {10.48550/ARXIV.1905.02175},
	
	url = {https://arxiv.org/abs/1905.02175},
	
	author = {Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
	
	keywords = {Machine Learning (stat.ML), Cryptography and Security (cs.CR), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {Adversarial Examples Are Not Bugs, They Are Features},
	
	publisher = {arXiv},
	
	year = {2019},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{simclr_chen_simple_2020,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {http://arxiv.org/abs/2002.05709},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	urldate = {2021-09-14},
	journal = {arXiv:2002.05709 [cs, stat]},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = jun,
	year = {2020},
	note = {arXiv: 2002.05709},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/kiyarash/Zotero/storage/MFP6RPQJ/Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf:application/pdf;arXiv.org Snapshot:/home/kiyarash/Zotero/storage/SYBHKA9N/2002.html:text/html},
}


@article{on_manifold_off_manifold,
	title = {Disentangling {Adversarial} {Robustness} and {Generalization}},
	url = {http://arxiv.org/abs/1812.00740},
	abstract = {Obtaining deep networks that are robust against adversarial examples and generalize well is an open problem. A recent hypothesis even states that both robust and accurate models are impossible, i.e., adversarial robustness and generalization are conflicting goals. In an effort to clarify the relationship between robustness and generalization, we assume an underlying, low-dimensional data manifold and show that: 1. regular adversarial examples leave the manifold; 2. adversarial examples constrained to the manifold, i.e., on-manifold adversarial examples, exist; 3. on-manifold adversarial examples are generalization errors, and on-manifold adversarial training boosts generalization; 4. regular robustness and generalization are not necessarily contradicting goals. These assumptions imply that both robust and accurate models are possible. However, different models (architectures, training strategies etc.) can exhibit different robustness and generalization characteristics. To confirm our claims, we present extensive experiments on synthetic data (with known manifold) as well as on EMNIST, Fashion-MNIST and CelebA.},
	urldate = {2021-11-17},
	journal = {arXiv:1812.00740 [cs, stat]},
	author = {Stutz, David and Hein, Matthias and Schiele, Bernt},
	month = apr,
	year = {2019},
	note = {arXiv: 1812.00740},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:/home/kiyarash/Zotero/storage/DTBKTBV3/Stutz et al. - 2019 - Disentangling Adversarial Robustness and Generaliz.pdf:application/pdf;arXiv.org Snapshot:/home/kiyarash/Zotero/storage/QDVRMQDS/1812.html:text/html},
}

